
# Parallel programming C++

## Contents

1. [Chapter I](#chapter-i) \
    1.1. [Introduction](#introduction)
2. [Chapter II](#chapter-ii) \
    2.1. [Information](#information)
3. [Chapter III](#chapter-iii) \
    3.1. [Part 1](#part-1-mpi-basics) \
    3.2. [Part 2](#part-2-bonus-part) \
    3.3. [Part 3](#part-3-midterm)

## Chapter I

![Parallel Bootcamp](../misc/images/abstract_lines.jpeg)

### Introduction

Добро пожаловать в MPI Bootcamp! Эта программа предназначена для подробного ознакомления с MPI, надежным API для параллельного программирования. MPI позволяет использовать возможности многоядерных процессоров и параллельных вычислений для повышения производительности приложений.

На протяжении всего курса вы рассмотрите различные аспекты MPI, стратегии распараллеливания, механизмы синхронизации и практические примеры. К концу обучения вы должны иметь четкое представление о распараллеливании кода и оптимизации его эффективности с помощью MPI.

## Chapter II

### Information

Open MPI (интерфейс передачи сообщений) — это платформа с открытым исходным кодом, предназначенная для параллельных и распределенных вычислений. Он позволяет разрабатывать высокопроизводительные масштабируемые приложения, позволяя нескольким процессам скоординировано взаимодействовать и сотрудничать. Вот некоторые основные термины, связанные с Open MPI:

Процесс MPI (ранг). В контексте MPI процесс — это экземпляр программы, способный выполняться независимо. Каждому процессу присваивается уникальный идентификатор, называемый рангом, который отличает его от других процессов.

Коммуникатор: Коммуникатор — это группа процессов MPI, которые могут взаимодействовать друг с другом. Он определяет объем связи и идентифицируется дескриптором коммуникатора.

Двухточечная связь: включает связь между двумя конкретными процессами MPI. Основными операциями для двухточечной связи являются MPI_Send (отправка данных) и MPI_Recv (прием данных).

Коллективное общение. Коллективное общение предполагает общение между группой процессов. Общие операции включают широковещательную рассылку (MPI_Bcast), разброс (MPI_Scatter), сбор (MPI_Gather) и сокращение (MPI_Reduce).

Блокирующие и неблокирующие операции. Операции MPI могут быть блокирующими и неблокирующими. Блокирующие операции, такие как MPI_Send и MPI_Recv, заставляют вызывающий процесс ждать завершения связи. Неблокирующие операции, такие как MPI_Isend и MPI_Irecv, позволяют программе продолжать выполнение во время обмена данными.

Библиотека MPI: Open MPI предоставляет набор библиотек и инструментов, реализующих стандарт MPI. Эти библиотеки облегчают разработку параллельных приложений, предоставляя согласованный интерфейс для связи и синхронизации.

Оболочка компилятора MPI: Open MPI включает оболочки компилятора (mpicc, mpic++, mpifort и т. д.), которые упрощают компиляцию и компоновку приложений MPI. Эти оболочки автоматически включают необходимые библиотеки и настройки MPI.

Ранг и размер MPI. Ранги MPI — это уникальные идентификаторы, присваиваемые каждому процессу в коммуникаторе, в диапазоне от 0 до (размер-1), где размер — это общее количество процессов в коммуникаторе.

## Chapter III

### Part 1. MPI Basics

#### Task 1

Написать программу, которая выводит "Hello world!" из каждого процесса.

#### Task 2

Написать программу, которая находит максимум массива используя многопоточность.

#### Task 3

Написать программу, которая вычисляет Пи методом Монте-карло.

#### Task 4

Написать программу, которая вычисляет среднее арифметическое среди положительных чисел массива.

#### Task 5

Написать программу, которая вычисляет скалярное произведение векторов.

#### Task 6

Написать программу, которая находит максимум и минимум матрицы.

#### Task 7

Написать программу, которая умножает матрицу на вектор при разделении данных по столбцам.

#### Task 8

Написать программу, которая реализует Scatter b Gather через Send и Recv.

#### Task 9

Написать программу, которая инветрирует массив.

#### Task 10

Написать программу, которая вычисляет время передачи для разных Send-ов.

#### Task 11

Написать программу, которая реализует циклическую передачу данных.

### Part 2. Bonus Part

#### Task 12

Написать программу, которая реализует передачу чисел по кругу для различных коммуникаторов.

#### Task 13

Написать программу, которая проверяет матрицы на симметричность.

#### Task 14

Написать программу, которая сортирует массив методом четной-нечетной перестановки.

### Part 3. Midterm

#### Task 15

Написать программу, которая реализует умножение матриц алгоритмом Кэннона.

#### Task 16

Написать программу, которая реализует умножение матриц алгоритмом Фокса.

#### Task 17

Написать программу, которая реализует сортировку матриц алгоритмом Шелла.

#### Task 18

Написать программу, которая реализует метод Гаусса.

#### Task 19

Написать программу, которая реализует быструю сортировк ус выбором ведущего элемента.

#### Task 20

Написать программу, которая реализует алгоритм Флойда-Уоршелла.

#### Task 21

Написать программу, которая реализует алгортим Краскала.

#### Task 22

Написать программу, которая реализует алгоритм прима.

#### Task 23

Написать программу, которая реализует алгоритм Кернигана-Лина.
